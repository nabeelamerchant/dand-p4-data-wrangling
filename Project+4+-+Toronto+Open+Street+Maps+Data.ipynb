{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Toronto Open Street Maps Data</h2>\n",
    "\n",
    "For this project I will be analyzing the Open Street Maps data pertaining to the city of Toronto in Canada as it my city of residence.\n",
    "\n",
    "- https://www.openstreetmap.org/relation/324211\n",
    "- http://overpass-api.de/api/map?bbox=-79.6694,43.5635,-79.0851,43.8736\n",
    "\n",
    "The original OSM XML file was exported from Overpass API using the following coordinates as the bounding area: -79.6694 to -79.0891 and 43.5635 to 43.8736.\n",
    "\n",
    "The file was 495MB in size, as seen by the following terminal output:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ls -hl toronto_map\n",
    "-rw-r--r--@ 1 NabeelaMerchant  staff   495M 22 Aug 20:33 toronto_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Data Wrangling</h3>\n",
    "\n",
    "In order to translate the xml file into a database, I first needed to take a look at the different nodes, \n",
    "node tags, etc. to get a  better sense of the data and see if anything needed to be fixed.\n",
    "\n",
    "To do that, I first created a sample file of the xml data that contained every 100th top level element. This \n",
    "helped me quickly scan through the data to find the most recurring errors. The code for this can be found in \n",
    "the \"1. make_sample_file.py\" file. Once I had written the scripts and audited the sample file, I transitioned to the original xml file.\n",
    "\n",
    "Next, I parsed through the xml file to identify the number of nodes, ways, etc. in the file. The code for this can be found in \"2. count_tags.py\". The output of the code for the original xml file is as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "{'bounds': 1,  \n",
    " 'member': 111493,  \n",
    " 'meta': 1,  \n",
    " 'nd': 2340335,  \n",
    " 'node': 2014881,  \n",
    " 'note': 1,  \n",
    " 'osm': 1,  \n",
    " 'relation': 5099,  \n",
    " 'tag': 2307930,  \n",
    " 'way': 343018} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then audited the sample file to search for inconsistencies in the street names, postal codes, and city names. This was an iterative process that involved identifying the different groups of street name endings, types of postal code recordings, and city names, and then creating a list of acceptable types to filter out any inconsistencies.\n",
    "\n",
    "These lists were then used later on with the original xml file to identify further inconsistencies and determine what changes needed to be made. The code for this can be found in \"3. audit_addresses.py\".\n",
    "\n",
    "Some of the errors I noticed were:\n",
    "- Variations of abbrevations/capitalizations for Street, Drive, Place, East, West, etc. (st., Pl., E., west)\n",
    "- Spelling mistakes (Terace instead of Terrace)\n",
    "- Complete addresses instead of just street names. This often included suite or floor numbers as well. (14th Avenue, Markham, Ont.; Lawrence Avenue West, 1st Floor)\n",
    "- Postal codes with a space missing between the 3rd and 4th character (M9W1J8 vs. M9W 1J8)\n",
    "- Postal codes with the last 3 characters missing (M9C)\n",
    "- Postal codes in the wrong case (m1g2l6)\n",
    "- Inconsistent city or town names (City of Brampton vs Brampton)\n",
    "- Incorrect city names (Torontoitalian)\n",
    "\n",
    "Once these errors were identified, I used the script \"4. update_osm.py\" to go through the xml file and make changes to fix the errors, mainly using dictionaries to map the incorrect entities to the correct ones. The corrections were then written to a new xml file. This had to be repeated a few times to catch stacked errors. \n",
    "\n",
    "For errors where the information was missing, such as with the truncated postal codes, I replaced the postal codes with 'Wrong Postal Code' to make it easy to identify later so the entry can be removed.\n",
    "\n",
    "I also noticed that the province was listed under the key 'state' or 'province', and changed the keys to all be 'province' as is the terminology in Canada.\n",
    "\n",
    "Once I was happy with the updated xml file, I used the script \"5. convert_osm_to_csv.py\" to convert the updated xml file to individual csv files for each table based on the schema provided. This resulted in 5 separate csv files that I could then import into the database. The files and their respective sizes (in MB) are as follows:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ls -hl nodes.csv nodes_tags.csv ways.csv ways_tags.csv ways_nodes.csv\n",
    "-rw-r--r--@ 1 NabeelaMerchant  staff   157M  1 Sep 17:59 nodes.csv\n",
    "-rw-r--r--@ 1 NabeelaMerchant  staff    38M  1 Sep 17:59 nodes_tags.csv\n",
    "-rw-r--r--@ 1 NabeelaMerchant  staff    19M  1 Sep 18:00 ways.csv\n",
    "-rw-r--r--@ 1 NabeelaMerchant  staff    52M  1 Sep 18:00 ways_nodes.csv\n",
    "-rw-r--r--@ 1 NabeelaMerchant  staff    41M  1 Sep 18:01 ways_tags.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Database Creation</h3>\n",
    "\n",
    "To create the database 'Toronto.db', I used sqlite3 in the command line to create the tables according to the \n",
    "schema provided and import the csv files. The commands used are listed in the text file 'sqlite3_commands.txt'.\n",
    "\n",
    "The size of the database is 281MB as seen below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ls -hl Toronto.db\n",
    "-rw-r--r--  1 NabeelaMerchant  staff   281M  7 Sep 11:34 Toronto.db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Updating the Database</h3>\n",
    "\n",
    "Now that the data was audited and imported, I could query it to find some interesting information about the city of Toronto. But before that, I needed to finish cleaning up the incorrect postal data. The tables 'ways_tags' and 'nodes_tags' contain the key 'postcodes' with the value 'Wrong Postal Code'. Using the following queries I identified the incorrect postal data and removed the values in the table:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select * from nodes_tags where key=\"postcode\" and value=\"Wrong Postal Code\";\n",
    "delete from nodes_tags where key=\"postcode\" and value=\"Wrong Postal Code\";\n",
    "\n",
    "select * from ways_tags where key=\"postcode\" and value=\"Wrong Postal Code\";\n",
    "delete from ways_tags where key=\"postcode\" and value=\"Wrong Postal Code\";"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Database Queries</h3>\n",
    "\n",
    "Now that the database had been cleaned up I moved on to querying. First I wanted to verify that the number of nodes and ways aligned with the values obtained from the python script \"2. count_tags.py\".\n",
    "\n",
    "- Number of nodes:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select count(id) from nodes;  \n",
    "    2014881"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Number of ways:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select count(id) from ways;  \n",
    "    343018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wanted to find the number of unique users that had contributed to this portion of the open street map, along with the top 5 contributing users:\n",
    "\n",
    "- Number of unique users:  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select count(users) from (select user as users from nodes group by user) as u;  \n",
    "    1682"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top 5 contributing users:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select user,count(user) from nodes group by user order by count(user) desc limit 5;  \n",
    "    andrewpmk,1371023\n",
    "    Kevo,151677\n",
    "    \"Victor Bielawski\",95708\n",
    "    Bootprint,57199\n",
    "    \"Mojgan Jadidi\",30997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then wanted to dig deeper into the different keys in the nodes_tags table, and identify the top keys and explore those further.\n",
    "\n",
    "- Top 10 keys in nodes_tags:    "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select key,count(value) from nodes_tags group by key order by count(value) desc limit 10;\n",
    "    source,203202\n",
    "    street,202844\n",
    "    housenumber,202783\n",
    "    city,177477\n",
    "    highway,50203\n",
    "    province,28275\n",
    "    country,28046\n",
    "    name,25176\n",
    "    amenity,18474\n",
    "    crossing,12722"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top 10 amenities in Toronto:  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select value, count(value) from nodes_tags where key='amenity' group by value order by count(value) desc limit 10;\n",
    "    restaurant,2072\n",
    "    fast_food,2010\n",
    "    bench,1612\n",
    "    post_box,1389\n",
    "    cafe,1073\n",
    "    parking,947\n",
    "    waste_basket,867\n",
    "    bank,706\n",
    "    pharmacy,498\n",
    "    telephone,494"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top 5 most popular restaurants:  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "create view rest_id as select id from nodes_tags as rest_id where value='restaurant';  \n",
    "\n",
    "select nodes_tags.value,count(nodes_tags.value) from rest_id,nodes_tags where rest_id.id=nodes_tags.id and nodes_tags.key='name' group by nodes_tags.value order by count(nodes_tags.value) desc limit 5;\n",
    "    \"Swiss Chalet\",34\n",
    "    \"Boston Pizza\",13\n",
    "    Eggsmart,12\n",
    "    \"Sunset Grill\",11\n",
    "    \"Wild Wing\",10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top 5 most popular cuisines:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "create view cuisine_id as select id from nodes_tags as cuisine_id where key='cuisine';\n",
    "\n",
    "select nodes_tags.value,count(nodes_tags.value) from nodes_tags,cuisine_id where nodes_tags.id=cuisine_id.id and key = 'cuisine' group by nodes_tags.value order by count(nodes_tags.value) desc limit 5;\n",
    "    coffee_shop,599\n",
    "    pizza,369\n",
    "    sandwich,333\n",
    "    burger,205\n",
    "    chinese,157"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the queries above we can see that coffee shops top the list of cuisines in Toronto. Given that Tim Hortons is known as Canada's go-to coffee shop, whereas in America, Starbucks is infamously known to be on every city corner, I wanted to dig into the actual popularity of different coffee shops in Toronto. As coffee shops are listed as restaurants but also under cafes, I need to explore both types of amenities to verify their populatiry.\n",
    "\n",
    "- Top 5 most popular coffee shops (restaurants):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "create view coffee_id as select id from nodes_tags as coffee_id where value=\"coffee_shop\";\n",
    "\n",
    "select nodes_tags.value,count(nodes_tags.value) from coffee_id,nodes_tags where coffee_id.id=nodes_tags.id and nodes_tags.key='name' group by nodes_tags.value order by count(nodes_tags.value) desc limit 5;\n",
    "    \"Tim Hortons\",235\n",
    "    \"Starbucks Coffee\",170\n",
    "    \"Second Cup\",71\n",
    "    \"Country Style\",16\n",
    "    \"Coffee Time\",8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top 5 most popular coffee shops (cafes):"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "create view cafe_id as select id from nodes_tags as cafe_id where value='cafe';\n",
    "\n",
    "select nodes_tags.value,count(nodes_tags.value) from cafe_id,nodes_tags where cafe_id.id=nodes_tags.id and nodes_tags.key='name' group by nodes_tags.value order by count(nodes_tags.value) desc limi\n",
    "    \"Tim Hortons\",276\n",
    "    \"Starbucks Coffee\",187\n",
    "    \"Second Cup\",76\n",
    "    \"Coffee Time\",48\n",
    "    \"Country Style\",25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both cases, there are more Tim Hortons outlets than Starbucks in Toronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Other ideas about the dataset</h3>\n",
    "\n",
    "In retrospect, having wrangled the xml data and converted in into a database to query, there are a few things I realised along the way and would have changed. While I included a snippet in my python code to change province information (state --> province and all values --> ON), not all the keys and values got caught and I realised that a few values were missed upon querying the database. I proceeded to fix the values after that (shown below), but would try to have a more robust systematic cleanse of the data in the python code itself.\n",
    "\n",
    "The queries below show the variations in province names in the nodes_tags table and the process of updating them to 'ON'."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select value,count(value) from nodes_tags where key='province' group by value;\n",
    "    ON,28255\n",
    "    On,5\n",
    "    Onatrio,5\n",
    "    Ont,1\n",
    "    Ontario,2\n",
    "    Onterio,1\n",
    "    on,2\n",
    "    ontario,4\n",
    "\n",
    "update nodes_tags set value='ON' where key='province' and value<>'ON';\n",
    "\n",
    "select value,count(value) from nodes_tags where key='province' group by value;\n",
    "    ON,28275"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When updating the province data for the 'ways_tags' table, I noticed that Florida was listed as a province. Upon further exploring the information associated with that ID I realised that the entry was for an automotive shop in Florida, USA and removed all the information associated with that ID as well. The queries for this, along with those used to update the remaining province variations are shown below:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select value,count(value) from ways_tags where key='province' group by value;\n",
    "    Florida,1\n",
    "    ON,9929\n",
    "    On,5\n",
    "    Ontario,9\n",
    "    Ontatio,1\n",
    "    on,2\n",
    "    ontario,1\n",
    "\n",
    "select * from ways_tags where key='province' and value=\"Florida\" group by value; \n",
    "    46583206,province,Florida,addr\n",
    "\n",
    "select * from ways_tags where id=\"46583206\";\n",
    "    46583206,phone,\"(800) 999-5880\",regular\n",
    "    46583206,housenumber,10600,addr\n",
    "    46583206,opening_hours,\"24/7 Available\",regular\n",
    "    46583206,postcode,\"339 13\",addr\n",
    "    46583206,name,\"Auto Glass America - Fort Myers\",regular\n",
    "    46583206,street,\"Colonial Boulevard\",addr\n",
    "    46583206,province,Florida,addr\n",
    "    46583206,city,\"Fort Myers\",addr\n",
    "    46583206,description,\"We provide an extensive list of windshield and auto glass replacement services throughout    \n",
    "    Tampa, Florida.\",regular\n",
    "    46583206,building,automotive,regular\n",
    "    46583206,shop,car,regular\n",
    "    46583206,website,http://www.auto-glassamerica.com/services/windshield-replacement-fort-myers,regular\n",
    "    46583206,service,repair,regular\n",
    "   \n",
    "select value,count(value) from ways_tags where key='province' group by value;\n",
    "    ON,9929\n",
    "    On,5\n",
    "    Ontario,9\n",
    "    Ontatio,1\n",
    "    on,2\n",
    "    ontario,1\n",
    "\n",
    "update ways_tags set value='ON' where key='province' and value<>'ON';\n",
    "\n",
    "select value,count(value) from ways_tags where key='province' group by value;\n",
    "    ON,9947"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following queries were used to change the key from 'state' to 'province'. No 'state' keys existed in the 'ways_tags' table so only the 'nodes_tags' table needed to be updated."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "select value,count(value) from ways_tags where key='state' group by value;\n",
    "\n",
    "select value,count(value) from nodes_tags where key='state' group by value;\n",
    "    ON,4\n",
    "    Ontario,5\n",
    "\n",
    "update nodes_tags set key='province' where key='state';"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the wrangling process for the province data even more robust, I would need to include the province key in the audit of the xml file. This would prevent overwriting incorrect data, such as the Florida entry. However, without writing code to specifically address the incorrect ids and remove them, I would still need to query the database for quick removal as shown above."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:py2]",
   "language": "python",
   "name": "conda-env-py2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
